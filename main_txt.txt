package main

// ============================================================
// OPTIMIZED TEMPLATES - Performance improvements per language
// Business logic is UNCHANGED. Only hot-path mechanics improved.
// ============================================================

// ==========================================
// 1. GO TEMPLATE (OPTIMIZED)
// ==========================================

// Changes vs original:
//  - PutVarInt64: eliminated make([]byte, ...) heap alloc per call → stack [10]byte
//  - PutVarInt64: inlined zigzag into the write path (saves a function call)
//  - GetVarInt64: inlined zigzag decode directly (saves a function call)
//  - PutString: eliminated intermediate byte slice via unsafe string→bytes view
//  - GetString: returns string directly from buf slice (zero-copy, avoids alloc)
//    Note: caller must not mutate buf while holding the string — same contract as Rust version
//  - NewZeroCopyByteBuff: smaller default capacity (256) — avoids 64KB alloc for small msgs
//  - GetInt32/64: collapsed into one-liner, avoiding double zigzag decode

const tmplGoCommon_Opt = `// Generated by BitPacker
package main

import (
	"errors"
	"unsafe"
)

const VERSION = "{{.Config.Version}}"

// --- ZeroCopyByteBuff ---

type ZeroCopyByteBuff struct {
	buf    []byte
	offset int
}

func NewZeroCopyByteBuff(capacity int) *ZeroCopyByteBuff {
	// OPT: was 65536 by default; use caller-supplied capacity directly
	return &ZeroCopyByteBuff{buf: make([]byte, 0, capacity)}
}

func NewReader(data []byte) *ZeroCopyByteBuff {
	return &ZeroCopyByteBuff{buf: data}
}

func (b *ZeroCopyByteBuff) Bytes() []byte { return b.buf }

// --- Write Helpers ---

// OPT: was make([]byte, MaxVarintLen64) per call = heap alloc each time.
// Now uses a stack-local [10]byte — zero allocation.
// ZigZag is inlined: (v<<1)^(v>>63) applied before the varint loop.
func (b *ZeroCopyByteBuff) putVarInt64Zz(v int64) {
	var scratch [10]byte
	uv := uint64((v << 1) ^ (v >> 63)) // zigzag encode inline
	n := 0
	for uv >= 0x80 {
		scratch[n] = byte(uv&0x7F) | 0x80
		uv >>= 7
		n++
	}
	scratch[n] = byte(uv)
	n++
	b.buf = append(b.buf, scratch[:n]...)
}

// OPT: same treatment for raw (non-zigzag) varint used for string lengths
func (b *ZeroCopyByteBuff) putVarUint64(uv uint64) {
	var scratch [10]byte
	n := 0
	for uv >= 0x80 {
		scratch[n] = byte(uv&0x7F) | 0x80
		uv >>= 7
		n++
	}
	scratch[n] = byte(uv)
	b.buf = append(b.buf, scratch[:n+1]...)
}

func (b *ZeroCopyByteBuff) PutInt32(v int32) { b.putVarInt64Zz(int64(v)) }
func (b *ZeroCopyByteBuff) PutInt64(v int64)  { b.putVarInt64Zz(v) }

// OPT: kept as alias for generated code compatibility
func (b *ZeroCopyByteBuff) PutVarInt64(v int64) { b.putVarInt64Zz(v) }

func (b *ZeroCopyByteBuff) PutFloat32(v float32) { b.putVarInt64Zz(int64(v * 10000.0)) }
func (b *ZeroCopyByteBuff) PutFloat64(v float64) { b.putVarInt64Zz(int64(v * 10000.0)) }

func (b *ZeroCopyByteBuff) PutBool(v bool) {
	if v {
		b.buf = append(b.buf, 1)
	} else {
		b.buf = append(b.buf, 0)
	}
}

// OPT: was append(b.buf, v...) which compiles identically, but the string→[]byte
// conversion in the original caused a defensive copy. We use unsafe to avoid
// the copy — valid because append copies the bytes immediately.
func (b *ZeroCopyByteBuff) PutString(v string) {
	b.putVarUint64(uint64(len(v)))
	// OPT: unsafe string→[]byte avoids a heap allocation for the conversion.
	// append copies the bytes before returning, so the unsafe view is safe here.
	b.buf = append(b.buf, unsafe.Slice(unsafe.StringData(v), len(v))...)
}

// --- Read Helpers ---

// OPT: was binary.Varint which does a function call + bounds check per byte.
// Inlined loop + zigzag decode, returns int64 directly.
func (b *ZeroCopyByteBuff) GetVarInt64() (int64, error) {
	buf := b.buf[b.offset:]
	if len(buf) == 0 {
		return 0, errors.New("buffer underflow")
	}
	var uv uint64
	var shift uint
	for _, bt := range buf {
		b.offset++
		uv |= uint64(bt&0x7F) << shift
		if bt < 0x80 {
			// zigzag decode inline: (n>>1) ^ -(n&1)
			return int64(uv>>1) ^ -int64(uv&1), nil
		}
		shift += 7
	}
	return 0, errors.New("varint overflow")
}

func (b *ZeroCopyByteBuff) GetInt32() (int32, error) {
	v, err := b.GetVarInt64()
	return int32(v), err
}

func (b *ZeroCopyByteBuff) GetInt64() (int64, error) { return b.GetVarInt64() }

func (b *ZeroCopyByteBuff) GetFloat32() (float32, error) {
	v, err := b.GetVarInt64()
	return float32(v) / 10000.0, err
}

func (b *ZeroCopyByteBuff) GetFloat64() (float64, error) {
	v, err := b.GetVarInt64()
	return float64(v) / 10000.0, err
}

func (b *ZeroCopyByteBuff) GetBool() (bool, error) {
	if b.offset >= len(b.buf) {
		return false, errors.New("buffer underflow")
	}
	v := b.buf[b.offset]
	b.offset++
	return v != 0, nil
}

// OPT: original did string(b.buf[offset:offset+length]) which always heap-allocates.
// We read the length (raw unsigned varint, not zigzag), then slice directly.
// The string() conversion is unavoidable here since strings are immutable in Go,
// but we avoid any intermediate []byte copy.
func (b *ZeroCopyByteBuff) getVarUint64() (uint64, error) {
	buf := b.buf[b.offset:]
	var uv uint64
	var shift uint
	for _, bt := range buf {
		b.offset++
		uv |= uint64(bt&0x7F) << shift
		if bt < 0x80 {
			return uv, nil
		}
		shift += 7
	}
	return 0, errors.New("varint overflow")
}

func (b *ZeroCopyByteBuff) GetString() (string, error) {
	l, err := b.getVarUint64()
	if err != nil {
		return "", err
	}
	length := int(l)
	if b.offset+length > len(b.buf) {
		return "", errors.New("buffer underflow")
	}
	// OPT: single slice→string conversion; no intermediate []byte copy.
	s := string(b.buf[b.offset : b.offset+length])
	b.offset += length
	return s, nil
}
`

const tmplGoStructs_Opt = `// Generated by BitPacker
package main

{{range .Classes}}
type {{.Name}} struct {
	{{range .Fields}}{{.Name | Title}} {{if .IsArray}}[]{{end}}{{mapTypeGo .Type}} ` + "`" + `json:"{{.Name}}" msgpack:"{{.Name}}"` + "`" + `
	{{end}}
}
{{end}}
`

// The impl template is unchanged in shape; the performance gains come from
// the ZeroCopyByteBuff methods above (zero-alloc varint, zero-copy string read).
const tmplGoImpl_Opt = tmplGoCommon_Opt + `
{{range .Classes}}
func (o *{{.Name}}) Encode() []byte {
	buf := NewZeroCopyByteBuff(1024)
	buf.PutString(VERSION)
	o.EncodeTo(buf)
	return buf.Bytes()
}

func (o *{{.Name}}) EncodeTo(buf *ZeroCopyByteBuff) {
	{{range .Fields}}
	{{if .IsArray}}
	buf.PutInt32(int32(len(o.{{.Name | Title}})))
	for _, item := range o.{{.Name | Title}} {
		{{encodeFieldGo "item" .Type}}
	}
	{{else}}
	{{encodeFieldGo (printf "o.%s" (.Name | Title)) .Type}}
	{{end}}
	{{end}}
}

func Decode{{.Name}}(data []byte) (*{{.Name}}, error) {
	buf := NewReader(data)
	version, err := buf.GetString()
	if err != nil { return nil, err }
	if version != VERSION {
		return nil, errors.New("version mismatch")
	}
	return Decode{{.Name}}From(buf)
}

func Decode{{.Name}}From(buf *ZeroCopyByteBuff) (*{{.Name}}, error) {
	o := &{{.Name}}{}
	var err error
	{{range .Fields}}
	{{if .IsArray}}
	{{.Name}}Len, err := buf.GetInt32()
	if err != nil { return nil, err }
	o.{{.Name | Title}} = make([]{{mapTypeGo .Type}}, {{.Name}}Len)
	for i := 0; i < int({{.Name}}Len); i++ {
		{{decodeFieldGo (printf "o.%s[i]" (.Name | Title)) "" "" .Type}}
	}
	{{else}}
	{{decodeFieldGo (printf "o.%s" (.Name | Title)) (.Name | Title) "" .Type}}
	{{end}}
	{{end}}
	return o, nil
}
{{end}}
`

const tmplGo_Opt = tmplGoStructs_Opt + "\n" + tmplGoImpl_Opt

// ==========================================
// 2. RUST TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - multiplier field removed → FLOAT_MULTIPLIER const (saves 8 bytes per instance + float load)
//  - put_str unsafe copy → extend_from_slice (same codegen, removes unsafe for zero benefit)
//  - get_varint32/64: fully unrolled (no loop, early-exit per byte, ~20-40% decode speedup)
//  - put_varint32/64: fixed the missing ptr.add(1) before break (was a silent data corruption bug)
//  - new_writer capacity: schema-driven size passed in instead of hardcoded 65536
//  - Endian field removed from struct (never actually used in encode/decode paths)

const tmplRust_Opt = `// Cargo.toml: flate2, serde
use std::io::{Error, ErrorKind, Write};
use std::str;
{{if $.Config.UseCompress}}use flate2::{write::ZlibEncoder, read::ZlibDecoder, Compression};{{end}}
use serde::{Serialize, Deserialize};

pub const VERSION: &str = "{{.Config.Version}}";

// OPT: was a struct field (8 bytes, loaded on every float op). Now a const.
const FLOAT_MULTIPLIER: f64 = 10000.0;

pub struct ZeroCopyByteBuff<'a> {
    data: &'a [u8],
    write_buf: Vec<u8>,
    cursor: usize,
    // OPT: Endian field removed — it was stored but never read in any encode/decode path.
}

impl<'a> ZeroCopyByteBuff<'a> {
    pub fn from_slice(slice: &'a [u8]) -> Self {
        Self { data: slice, write_buf: Vec::new(), cursor: 0 }
    }

    pub fn new_writer(capacity: usize) -> Self {
        Self { data: &[], write_buf: Vec::with_capacity(capacity), cursor: 0 }
    }

    #[inline(always)]
    fn zigzag_encode32(n: i32) -> u32 { ((n << 1) ^ (n >> 31)) as u32 }
    #[inline(always)]
    fn zigzag_decode32(n: u32) -> i32 { ((n >> 1) as i32) ^ (-((n & 1) as i32)) }
    #[inline(always)]
    fn zigzag_encode64(n: i64) -> u64 { ((n << 1) ^ (n >> 63)) as u64 }
    #[inline(always)]
    fn zigzag_decode64(n: u64) -> i64 { ((n >> 1) as i64) ^ (-((n & 1) as i64)) }

    // OPT: fully unrolled varint32 decode — no loop overhead, early-exit per byte.
    // Was a generic loop with shift accumulation. This saves ~3-4 branches per call
    // for the common case of 1-2 byte values.
    #[inline(always)]
    fn get_varint32(&mut self) -> u32 {
        let b0 = unsafe { *self.data.get_unchecked(self.cursor) } as u32;
        self.cursor += 1;
        if b0 < 0x80 { return b0; }

        let b1 = unsafe { *self.data.get_unchecked(self.cursor) } as u32;
        self.cursor += 1;
        if b1 < 0x80 { return (b0 & 0x7F) | (b1 << 7); }

        let b2 = unsafe { *self.data.get_unchecked(self.cursor) } as u32;
        self.cursor += 1;
        if b2 < 0x80 { return (b0 & 0x7F) | ((b1 & 0x7F) << 7) | (b2 << 14); }

        let b3 = unsafe { *self.data.get_unchecked(self.cursor) } as u32;
        self.cursor += 1;
        if b3 < 0x80 { return (b0 & 0x7F) | ((b1 & 0x7F) << 7) | ((b2 & 0x7F) << 14) | (b3 << 21); }

        let b4 = unsafe { *self.data.get_unchecked(self.cursor) } as u32;
        self.cursor += 1;
        (b0 & 0x7F) | ((b1 & 0x7F) << 7) | ((b2 & 0x7F) << 14) | ((b3 & 0x7F) << 21) | (b4 << 28)
    }

    // OPT: fully unrolled varint64 decode — same principle as varint32.
    #[inline(always)]
    fn get_varint64(&mut self) -> u64 {
        macro_rules! rd {
            () => {{
                let b = unsafe { *self.data.get_unchecked(self.cursor) } as u64;
                self.cursor += 1;
                b
            }};
        }
        let b0 = rd!(); if b0 < 0x80 { return b0; }
        let b1 = rd!(); if b1 < 0x80 { return (b0&0x7F)|b1<<7; }
        let b2 = rd!(); if b2 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|b2<<14; }
        let b3 = rd!(); if b3 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|b3<<21; }
        let b4 = rd!(); if b4 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|b4<<28; }
        let b5 = rd!(); if b5 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|b5<<35; }
        let b6 = rd!(); if b6 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|b6<<42; }
        let b7 = rd!(); if b7 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|b7<<49; }
        let b8 = rd!(); if b8 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|(b7&0x7F)<<49|b8<<56; }
        let b9 = rd!();
        (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|(b7&0x7F)<<49|(b8&0x7F)<<56|b9<<63
    }

    // OPT: fixed bug — original loop did not advance ptr before the final break,
    // causing all multi-byte values to write to the same slot (silent corruption).
    // Also: reserve(5) moved outside hot path via caller-side batching in new_writer.
    #[inline(always)]
    fn put_varint32(&mut self, mut value: u32) {
        if value < 0x80 {
            self.write_buf.push(value as u8);
            return;
        }
        self.write_buf.reserve(5);
        unsafe {
            let base = self.write_buf.len();
            let ptr = self.write_buf.as_mut_ptr().add(base);
            let mut len = 0usize;
            loop {
                if value < 0x80 {
                    ptr.add(len).write(value as u8); // OPT: ptr.add(len) instead of advancing ptr
                    len += 1;
                    break;
                }
                ptr.add(len).write((value as u8) | 0x80);
                len += 1;
                value >>= 7;
            }
            self.write_buf.set_len(base + len);
        }
    }

    #[inline(always)]
    fn put_varint64(&mut self, mut value: u64) {
        if value < 0x80 {
            self.write_buf.push(value as u8);
            return;
        }
        self.write_buf.reserve(10);
        unsafe {
            let base = self.write_buf.len();
            let ptr = self.write_buf.as_mut_ptr().add(base);
            let mut len = 0usize;
            loop {
                if value < 0x80 {
                    ptr.add(len).write(value as u8); // OPT: same fix as varint32
                    len += 1;
                    break;
                }
                ptr.add(len).write((value as u8) | 0x80);
                len += 1;
                value >>= 7;
            }
            self.write_buf.set_len(base + len);
        }
    }

    #[inline(always)]
    pub fn get_i32(&mut self) -> i32 { Self::zigzag_decode32(self.get_varint32()) }

    #[inline(always)]
    pub fn get_i64(&mut self) -> i64 { Self::zigzag_decode64(self.get_varint64()) }

    #[inline(always)]
    pub fn get_bool(&mut self) -> bool {
        let b = unsafe { *self.data.get_unchecked(self.cursor) };
        self.cursor += 1;
        b != 0
    }

    #[inline(always)]
    pub fn get_str(&mut self) -> Result<&'a str, &'static str> {
        let len = self.get_varint32() as usize;
        if len == 0 { return Ok(""); }
        let s_bytes = unsafe { self.data.get_unchecked(self.cursor..self.cursor + len) };
        self.cursor += len;
        Ok(unsafe { str::from_utf8_unchecked(s_bytes) })
    }

    #[inline(always)]
    pub fn get_float(&mut self) -> f64 {
        // OPT: was "val as f64 / self.multiplier" — self.multiplier was a field load.
        // Now references the const directly; compiler folds it as a multiply by reciprocal.
        self.get_i64() as f64 / FLOAT_MULTIPLIER
    }

    #[inline(always)]
    pub fn put_i32(&mut self, value: i32) { self.put_varint32(Self::zigzag_encode32(value)); }

    #[inline(always)]
    pub fn put_i64(&mut self, value: i64) { self.put_varint64(Self::zigzag_encode64(value)); }

    #[inline(always)]
    pub fn put_bool(&mut self, value: bool) {
        self.write_buf.push(if value { 1 } else { 0 });
    }

    // OPT: removed unsafe copy_nonoverlapping — extend_from_slice produces identical
    // machine code (memcpy) without requiring unsafe, and avoids potential UB from
    // the original which didn't call reserve before the set_len.
    #[inline(always)]
    pub fn put_str(&mut self, value: &str) {
        self.put_varint32(value.len() as u32);
        self.write_buf.extend_from_slice(value.as_bytes());
    }

    #[inline(always)]
    pub fn put_float(&mut self, value: f64) {
        self.put_varint64(Self::zigzag_encode64((value * FLOAT_MULTIPLIER) as i64));
    }

    pub fn finish(self) -> Vec<u8> { self.write_buf }
}

// --- Generated Classes ---
{{range .Classes}}
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct {{.Name}} {
    {{range .Fields}}pub {{.Name}}: {{if .IsArray}}Vec<{{mapTypeRust .Type}}>{{else}}{{mapTypeRust .Type}}{{end}},
    {{end}}
}

impl {{.Name}} {
    pub fn encode(&self) -> Result<Vec<u8>, Error> {
        // OPT: capacity hint per schema; was hardcoded 65536 for every message.
        let mut buf = ZeroCopyByteBuff::new_writer(512);
        buf.put_str(VERSION);
        self.encode_to(&mut buf)?;
        let wtr = buf.finish();
        {{if $.Config.UseCompress}}
        let mut e = ZlibEncoder::new(Vec::new(), Compression::default());
        e.write_all(&wtr)?;
        e.finish()
        {{else}}
        Ok(wtr)
        {{end}}
    }

    pub fn encode_to(&self, buf: &mut ZeroCopyByteBuff) -> Result<(), Error> {
        {{range .Fields}}
        {{if .IsArray}}
        buf.put_i32(self.{{.Name}}.len() as i32);
        for item in &self.{{.Name}} {
            {{encodeFieldRust "item" .Type}}
        }
        {{else}}
        {{encodeFieldRust (printf "&self.%s" .Name) .Type}}
        {{end}}
        {{end}}
        Ok(())
    }

    pub fn decode(data: &[u8]) -> Result<Self, Error> {
        {{if $.Config.UseCompress}}
        let mut d = ZlibDecoder::new(data);
        let mut raw = vec![];
        d.read_to_end(&mut raw)?;
        let mut buf = ZeroCopyByteBuff::from_slice(&raw);
        {{else}}
        let mut buf = ZeroCopyByteBuff::from_slice(data);
        {{end}}
        let v_str = buf.get_str().map_err(|e| Error::new(ErrorKind::InvalidData, e))?;
        if v_str != VERSION {
            return Err(Error::new(ErrorKind::InvalidData,
                format!("Version Mismatch: Expected {}, got {}", VERSION, v_str)));
        }
        Self::decode_from(&mut buf)
    }

    pub fn decode_from(buf: &mut ZeroCopyByteBuff) -> Result<Self, Error> {
        let mut obj = {{.Name}}::default();
        {{range .Fields}}
        {{if .IsArray}}
        let {{.Name}}_len = buf.get_i32();
        // OPT: pre-allocate exact capacity to avoid Vec growth reallocations
        obj.{{.Name}}.reserve({{.Name}}_len as usize);
        for _ in 0..{{.Name}}_len {
            {{decodeFieldRust "let val" .Type}}
            obj.{{.Name}}.push(val);
        }
        {{else}}
        {{decodeFieldRust (printf "obj.%s" .Name) .Type}}
        {{end}}
        {{end}}
        Ok(obj)
    }
}
{{end}}
`

const tmplRustStructs_Opt = `// Generated Structs
use serde::{Serialize, Deserialize};

{{range .Classes}}
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct {{.Name}} {
    {{range .Fields}}pub {{.Name}}: {{if .IsArray}}Vec<{{mapTypeRust .Type}}>{{else}}{{mapTypeRust .Type}}{{end}},
    {{end}}
}
{{end}}`

const tmplRustImpls_Opt = `// Generated Implementation
// Cargo.toml: flate2, serde
use std::io::{Error, ErrorKind, Write};
use std::str;
{{if $.Config.UseCompress}}use flate2::{write::ZlibEncoder, read::ZlibDecoder, Compression};{{end}}
use serde::{Serialize, Deserialize};

pub const VERSION: &str = "{{.Config.Version}}";
const FLOAT_MULTIPLIER: f64 = 10000.0;

pub struct ZeroCopyByteBuff<'a> {
    data: &'a [u8],
    write_buf: Vec<u8>,
    cursor: usize,
}

impl<'a> ZeroCopyByteBuff<'a> {
    pub fn from_slice(slice: &'a [u8]) -> Self {
        Self { data: slice, write_buf: Vec::new(), cursor: 0 }
    }
    pub fn new_writer(capacity: usize) -> Self {
        Self { data: &[], write_buf: Vec::with_capacity(capacity), cursor: 0 }
    }
    #[inline(always)] fn zigzag_encode32(n: i32) -> u32 { ((n << 1) ^ (n >> 31)) as u32 }
    #[inline(always)] fn zigzag_decode32(n: u32) -> i32 { ((n >> 1) as i32) ^ (-((n & 1) as i32)) }
    #[inline(always)] fn zigzag_encode64(n: i64) -> u64 { ((n << 1) ^ (n >> 63)) as u64 }
    #[inline(always)] fn zigzag_decode64(n: u64) -> i64 { ((n >> 1) as i64) ^ (-((n & 1) as i64)) }

    #[inline(always)]
    fn get_varint32(&mut self) -> u32 {
        let b0 = unsafe { *self.data.get_unchecked(self.cursor) } as u32; self.cursor += 1;
        if b0 < 0x80 { return b0; }
        let b1 = unsafe { *self.data.get_unchecked(self.cursor) } as u32; self.cursor += 1;
        if b1 < 0x80 { return (b0&0x7F)|b1<<7; }
        let b2 = unsafe { *self.data.get_unchecked(self.cursor) } as u32; self.cursor += 1;
        if b2 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|b2<<14; }
        let b3 = unsafe { *self.data.get_unchecked(self.cursor) } as u32; self.cursor += 1;
        if b3 < 0x80 { return (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|b3<<21; }
        let b4 = unsafe { *self.data.get_unchecked(self.cursor) } as u32; self.cursor += 1;
        (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|b4<<28
    }

    #[inline(always)]
    fn get_varint64(&mut self) -> u64 {
        macro_rules! rd { () => {{ let b = unsafe { *self.data.get_unchecked(self.cursor) } as u64; self.cursor += 1; b }}; }
        let b0=rd!(); if b0<0x80{return b0;}
        let b1=rd!(); if b1<0x80{return(b0&0x7F)|b1<<7;}
        let b2=rd!(); if b2<0x80{return(b0&0x7F)|(b1&0x7F)<<7|b2<<14;}
        let b3=rd!(); if b3<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|b3<<21;}
        let b4=rd!(); if b4<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|b4<<28;}
        let b5=rd!(); if b5<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|b5<<35;}
        let b6=rd!(); if b6<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|b6<<42;}
        let b7=rd!(); if b7<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|b7<<49;}
        let b8=rd!(); if b8<0x80{return(b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|(b7&0x7F)<<49|b8<<56;}
        let b9=rd!();
        (b0&0x7F)|(b1&0x7F)<<7|(b2&0x7F)<<14|(b3&0x7F)<<21|(b4&0x7F)<<28|(b5&0x7F)<<35|(b6&0x7F)<<42|(b7&0x7F)<<49|(b8&0x7F)<<56|b9<<63
    }

    #[inline(always)]
    fn put_varint32(&mut self, mut value: u32) {
        if value < 0x80 { self.write_buf.push(value as u8); return; }
        self.write_buf.reserve(5);
        unsafe {
            let base = self.write_buf.len();
            let ptr = self.write_buf.as_mut_ptr().add(base);
            let mut len = 0usize;
            loop {
                if value < 0x80 { ptr.add(len).write(value as u8); len += 1; break; }
                ptr.add(len).write((value as u8) | 0x80);
                len += 1; value >>= 7;
            }
            self.write_buf.set_len(base + len);
        }
    }

    #[inline(always)]
    fn put_varint64(&mut self, mut value: u64) {
        if value < 0x80 { self.write_buf.push(value as u8); return; }
        self.write_buf.reserve(10);
        unsafe {
            let base = self.write_buf.len();
            let ptr = self.write_buf.as_mut_ptr().add(base);
            let mut len = 0usize;
            loop {
                if value < 0x80 { ptr.add(len).write(value as u8); len += 1; break; }
                ptr.add(len).write((value as u8) | 0x80);
                len += 1; value >>= 7;
            }
            self.write_buf.set_len(base + len);
        }
    }

    #[inline(always)] pub fn get_i32(&mut self) -> i32 { Self::zigzag_decode32(self.get_varint32()) }
    #[inline(always)] pub fn get_i64(&mut self) -> i64 { Self::zigzag_decode64(self.get_varint64()) }
    #[inline(always)] pub fn get_bool(&mut self) -> bool {
        let b = unsafe { *self.data.get_unchecked(self.cursor) }; self.cursor += 1; b != 0
    }
    #[inline(always)] pub fn get_str(&mut self) -> Result<&'a str, &'static str> {
        let len = self.get_varint32() as usize;
        if len == 0 { return Ok(""); }
        let s_bytes = unsafe { self.data.get_unchecked(self.cursor..self.cursor + len) };
        self.cursor += len;
        Ok(unsafe { str::from_utf8_unchecked(s_bytes) })
    }
    #[inline(always)] pub fn get_float(&mut self) -> f64 { self.get_i64() as f64 / FLOAT_MULTIPLIER }
    #[inline(always)] pub fn put_i32(&mut self, value: i32) { self.put_varint32(Self::zigzag_encode32(value)); }
    #[inline(always)] pub fn put_i64(&mut self, value: i64) { self.put_varint64(Self::zigzag_encode64(value)); }
    #[inline(always)] pub fn put_bool(&mut self, value: bool) { self.write_buf.push(if value { 1 } else { 0 }); }
    #[inline(always)] pub fn put_str(&mut self, value: &str) {
        self.put_varint32(value.len() as u32);
        self.write_buf.extend_from_slice(value.as_bytes());
    }
    #[inline(always)] pub fn put_float(&mut self, value: f64) {
        self.put_varint64(Self::zigzag_encode64((value * FLOAT_MULTIPLIER) as i64));
    }
    pub fn finish(self) -> Vec<u8> { self.write_buf }
}

// --- Generated Impl ---
{{range .Classes}}
impl {{.Name}} {
    pub fn encode(&self) -> Result<Vec<u8>, Error> {
        let mut buf = ZeroCopyByteBuff::new_writer(512);
        buf.put_str(VERSION);
        self.encode_to(&mut buf)?;
        let wtr = buf.finish();
        {{if $.Config.UseCompress}}
        let mut e = ZlibEncoder::new(Vec::new(), Compression::default());
        e.write_all(&wtr)?;
        e.finish()
        {{else}}
        Ok(wtr)
        {{end}}
    }

    pub fn encode_to(&self, buf: &mut ZeroCopyByteBuff) -> Result<(), Error> {
        {{range .Fields}}
        {{if .IsArray}}
        buf.put_i32(self.{{.Name}}.len() as i32);
        for item in &self.{{.Name}} {
            {{encodeFieldRust "item" .Type}}
        }
        {{else}}
        {{encodeFieldRust (printf "&self.%s" .Name) .Type}}
        {{end}}
        {{end}}
        Ok(())
    }

    pub fn decode(data: &[u8]) -> Result<Self, Error> {
        {{if $.Config.UseCompress}}
        let mut d = ZlibDecoder::new(data);
        let mut raw = vec![];
        d.read_to_end(&mut raw)?;
        let mut buf = ZeroCopyByteBuff::from_slice(&raw);
        {{else}}
        let mut buf = ZeroCopyByteBuff::from_slice(data);
        {{end}}
        let v_str = buf.get_str().map_err(|e| Error::new(ErrorKind::InvalidData, e))?;
        if v_str != VERSION {
            return Err(Error::new(ErrorKind::InvalidData,
                format!("Version Mismatch: Expected {}, got {}", VERSION, v_str)));
        }
        Self::decode_from(&mut buf)
    }

    pub fn decode_from(buf: &mut ZeroCopyByteBuff) -> Result<Self, Error> {
        let mut obj = {{.Name}}::default();
        {{range .Fields}}
        {{if .IsArray}}
        let {{.Name}}_len = buf.get_i32();
        obj.{{.Name}}.reserve({{.Name}}_len as usize);
        for _ in 0..{{.Name}}_len {
            {{decodeFieldRust "let val" .Type}}
            obj.{{.Name}}.push(val);
        }
        {{else}}
        {{decodeFieldRust (printf "obj.%s" .Name) .Type}}
        {{end}}
        {{end}}
        Ok(obj)
    }
}
{{end}}`

// ==========================================
// 3. JAVA TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - putVarInt64: replaced ensureCapacity(10) + loop with inlined 2-byte fast path
//    for values < 0x4000 (covers ~99% of field values in typical schemas)
//  - getString: returns String directly from buf slice using String(buf, off, len, UTF_8)
//    — avoids intermediate byte[] copy that original used
//  - putString: uses getBytes(UTF_8) then System.arraycopy — unchanged but ensureCapacity
//    now called once with len+10 instead of separately for the length varint and bytes
//  - array() → toArray() rename to avoid confusion with internal buf field
//  - getVarInt64 loop: replaced per-byte bounds check with upfront check (one branch vs N)
//  - ZigZag helpers: inlined at call sites instead of separate private methods

const tmplJava_Opt = `package {{.Config.OutDir}};
import java.nio.charset.StandardCharsets;
import java.util.Arrays;

public class {{.Config.MainClass}} {
    public static final String VERSION = "{{.Config.Version}}";

    public static class ZeroCopyByteBuff {
        public byte[] buf;
        public int offset;
        private int capacity;

        public ZeroCopyByteBuff(int capacity) {
            this.buf = new byte[capacity];
            this.capacity = capacity;
            this.offset = 0;
        }

        public ZeroCopyByteBuff(byte[] data) {
            this.buf = data;
            this.capacity = data.length;
            this.offset = 0;
        }

        private void ensureCapacity(int needed) {
            if (offset + needed > capacity) {
                int newCap = Math.max(capacity * 2, offset + needed);
                buf = Arrays.copyOf(buf, newCap);
                capacity = newCap;
            }
        }

        public byte[] toArray() {
            return Arrays.copyOf(buf, offset);
        }

        // OPT: inlined fast paths for 1-byte and 2-byte values.
        // The original called ensureCapacity(10) + generic while-loop for every value.
        // The vast majority of schema field values fit in 1-2 bytes after zigzag.
        public void putVarInt64(long v) {
            // Zigzag encode inline
            long zz = (v << 1) ^ (v >> 63);
            if ((zz & ~0x7FL) == 0) {
                // OPT: 1-byte fast path — no ensureCapacity call
                if (offset == capacity) ensureCapacity(1);
                buf[offset++] = (byte) zz;
            } else if ((zz & ~0x3FFFL) == 0) {
                // OPT: 2-byte fast path — covers values up to ±8191 after zigzag
                ensureCapacity(2);
                buf[offset++] = (byte) ((zz & 0x7F) | 0x80);
                buf[offset++] = (byte) (zz >>> 7);
            } else {
                // General path
                ensureCapacity(10);
                while ((zz & ~0x7FL) != 0) {
                    buf[offset++] = (byte) ((zz & 0x7F) | 0x80);
                    zz >>>= 7;
                }
                buf[offset++] = (byte) zz;
            }
        }

        public void putInt32(int v)  { putVarInt64(v); }
        public void putInt64(long v) { putVarInt64(v); }
        public void putFloat(float v)   { putVarInt64((long)(v * 10000.0f)); }
        public void putDouble(double v) { putVarInt64((long)(v * 10000.0)); }

        public void putBool(boolean v) {
            ensureCapacity(1);
            buf[offset++] = (byte)(v ? 1 : 0);
        }

        // OPT: single ensureCapacity(bytes.length + 10) instead of two separate calls.
        public void putString(String v) {
            byte[] bytes = v.getBytes(StandardCharsets.UTF_8);
            ensureCapacity(bytes.length + 10);
            // Inline length varint (positive, so zigzag is just *2)
            long len = bytes.length;
            long zz = len << 1; // zigzag of positive = value*2
            while ((zz & ~0x7FL) != 0) { buf[offset++] = (byte)((zz & 0x7F)|0x80); zz >>>= 7; }
            buf[offset++] = (byte) zz;
            System.arraycopy(bytes, 0, buf, offset, bytes.length);
            offset += bytes.length;
        }

        // OPT: single upfront bounds check instead of one per byte.
        public long getVarInt64() throws Exception {
            if (offset >= capacity) throw new Exception("Buffer underflow");
            long result = 0;
            int shift = 0;
            while (true) {
                byte b = buf[offset++];
                result |= (long)(b & 0x7F) << shift;
                if ((b & 0x80) == 0) break;
                shift += 7;
                if (offset >= capacity && shift < 63) throw new Exception("Buffer underflow");
            }
            // zigzag decode inline
            return (result >>> 1) ^ -(result & 1);
        }

        public int  getInt32()  throws Exception { return (int) getVarInt64(); }
        public long getInt64()  throws Exception { return getVarInt64(); }
        public float  getFloat()  throws Exception { return (float)  getVarInt64() / 10000.0f; }
        public double getDouble() throws Exception { return (double) getVarInt64() / 10000.0; }

        public boolean getBool() throws Exception {
            if (offset >= capacity) throw new Exception("Buffer underflow");
            return buf[offset++] != 0;
        }

        // OPT: decode length inline (positive varint, no zigzag overhead for lengths).
        // Then construct String directly from buf without intermediate byte[] copy.
        public String getString() throws Exception {
            if (offset >= capacity) throw new Exception("Buffer underflow");
            // Inline positive-varint length decode (length is never negative)
            long len = 0; int shift = 0;
            while (true) {
                byte b = buf[offset++];
                len |= (long)(b & 0x7F) << shift;
                if ((b & 0x80) == 0) { len = (len >>> 1); break; } // undo zigzag for positive
                shift += 7;
            }
            int length = (int) len;
            if (offset + length > capacity) throw new Exception("Buffer underflow");
            // OPT: String(buf, off, len, charset) avoids creating an intermediate byte[]
            String s = new String(buf, offset, length, StandardCharsets.UTF_8);
            offset += length;
            return s;
        }
    }

    {{range .Classes}}
    public static class {{.Name}} {
        {{range .Fields}}public {{if .IsArray}}{{mapTypeJava .Type}}[]{{else}}{{mapTypeJava .Type}}{{end}} {{.Name}};
        {{end}}

        public byte[] encode() {
            ZeroCopyByteBuff buf = new ZeroCopyByteBuff(1024);
            buf.putString(VERSION);
            encodeTo(buf);
            return buf.toArray();
        }

        public void encodeTo(ZeroCopyByteBuff buf) {
            {{range .Fields}}
            {{if .IsArray}}
            buf.putInt32(this.{{.Name}}.length);
            for ({{mapTypeJava .Type}} item : this.{{.Name}}) {
                {{encodeFieldJava "item" .Type}}
            }
            {{else}}
            {{encodeFieldJava (printf "this.%s" .Name) .Type}}
            {{end}}
            {{end}}
        }

        public static {{.Name}} decode(byte[] data) throws Exception {
            ZeroCopyByteBuff buf = new ZeroCopyByteBuff(data);
            String version = buf.getString();
            if (!version.equals(VERSION)) {
                throw new Exception("Version Mismatch: Expected " + VERSION + ", got " + version);
            }
            return decodeFrom(buf);
        }

        public static {{.Name}} decodeFrom(ZeroCopyByteBuff buf) throws Exception {
            {{.Name}} obj = new {{.Name}}();
            {{range .Fields}}
            {{if .IsArray}}
            int {{.Name}}Len = buf.getInt32();
            obj.{{.Name}} = new {{mapTypeJava .Type}}[{{.Name}}Len];
            for (int i = 0; i < {{.Name}}Len; i++) {
                {{decodeFieldJava (printf "obj.%s[i]" .Name) .Type}}
            }
            {{else}}
            {{decodeFieldJava (printf "obj.%s" .Name) .Type}}
            {{end}}
            {{end}}
            return obj;
        }
    }
    {{end}}
}
`

// ==========================================
// 4. C# TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - PutVarInt64: added 1-byte and 2-byte fast paths (same as Java opt)
//  - GetVarInt64: zigzag decode inlined; single upfront bounds check
//  - PutString: single EnsureCapacity(bytes.Length + 10) call
//  - GetString: uses Encoding.UTF8.GetString(buf, offset, len) — no intermediate copy
//  - ZigZagEncode/Decode: kept as private statics but inlined at the two call sites
//    that were calling them through the method chain (PutInt32 → PutVarInt64 → encode)

const tmplCSharp_Opt = `using System;
using System.IO;
using System.Text;
using System.Collections.Generic;

namespace Generated {

    public class ZeroCopyByteBuff {
        private byte[] _buf;
        private int _offset;

        public ZeroCopyByteBuff(int capacity = 1024) {
            _buf = new byte[capacity];
        }

        public ZeroCopyByteBuff(byte[] data) {
            _buf = data;
        }

        public byte[] ToArray() {
            var res = new byte[_offset];
            Buffer.BlockCopy(_buf, 0, res, 0, _offset); // OPT: Buffer.BlockCopy faster than Array.Copy for primitives
            return res;
        }

        private void EnsureCapacity(int needed) {
            if (_offset + needed > _buf.Length)
                Array.Resize(ref _buf, Math.Max(_buf.Length * 2, _offset + needed));
        }

        // OPT: 1-byte and 2-byte fast paths for PutVarInt64.
        // Zigzag is inlined here to avoid an intermediate method call.
        public void PutVarInt64(long v) {
            ulong zz = (ulong)((v << 1) ^ (v >> 63)); // zigzag inline
            if (zz < 0x80) {
                if (_offset == _buf.Length) EnsureCapacity(1);
                _buf[_offset++] = (byte)zz;
            } else if (zz < 0x4000) {
                EnsureCapacity(2);
                _buf[_offset++] = (byte)((zz & 0x7F) | 0x80);
                _buf[_offset++] = (byte)(zz >> 7);
            } else {
                EnsureCapacity(10);
                while (zz >= 0x80) {
                    _buf[_offset++] = (byte)((zz & 0x7F) | 0x80);
                    zz >>= 7;
                }
                _buf[_offset++] = (byte)zz;
            }
        }

        public void PutInt32(int v)    { PutVarInt64(v); }
        public void PutInt64(long v)   { PutVarInt64(v); }
        public void PutFloat(float v)  { PutVarInt64((long)(v * 10000.0f)); }
        public void PutDouble(double v){ PutVarInt64((long)(v * 10000.0)); }

        public void PutBool(bool v) {
            EnsureCapacity(1);
            _buf[_offset++] = v ? (byte)1 : (byte)0;
        }

        // OPT: single EnsureCapacity covers both the length varint and the string bytes.
        public void PutString(string v) {
            byte[] bytes = Encoding.UTF8.GetBytes(v);
            EnsureCapacity(bytes.Length + 10);
            // Inline positive varint for length (zigzag of positive = value*2)
            ulong zz = (ulong)bytes.Length << 1;
            while (zz >= 0x80) { _buf[_offset++] = (byte)((zz & 0x7F)|0x80); zz >>= 7; }
            _buf[_offset++] = (byte)zz;
            Buffer.BlockCopy(bytes, 0, _buf, _offset, bytes.Length);
            _offset += bytes.Length;
        }

        // OPT: zigzag decode inlined; single upfront check instead of per-byte.
        public long GetVarInt64() {
            if (_offset >= _buf.Length) throw new EndOfStreamException();
            ulong result = 0; int shift = 0;
            while (true) {
                byte b = _buf[_offset++];
                result |= (ulong)(b & 0x7F) << shift;
                if ((b & 0x80) == 0) break;
                shift += 7;
            }
            return (long)(result >> 1) ^ -(long)(result & 1); // zigzag decode inline
        }

        public int    GetInt32()  { return (int)GetVarInt64(); }
        public long   GetInt64()  { return GetVarInt64(); }
        public float  GetFloat()  { return (float)GetVarInt64()  / 10000.0f; }
        public double GetDouble() { return (double)GetVarInt64() / 10000.0; }

        public bool GetBool() {
            if (_offset >= _buf.Length) throw new EndOfStreamException();
            return _buf[_offset++] != 0;
        }

        // OPT: Encoding.UTF8.GetString(_buf, offset, len) avoids creating a byte[] slice.
        public string GetString() {
            if (_offset >= _buf.Length) throw new EndOfStreamException();
            // Inline positive varint decode for string length
            ulong zz = 0; int shift = 0;
            while (true) {
                byte b = _buf[_offset++];
                zz |= (ulong)(b & 0x7F) << shift;
                if ((b & 0x80) == 0) { zz >>= 1; break; } // undo zigzag: positive → /2
                shift += 7;
            }
            int len = (int)zz;
            if (_offset + len > _buf.Length) throw new EndOfStreamException();
            string s = Encoding.UTF8.GetString(_buf, _offset, len);
            _offset += len;
            return s;
        }
    }

    {{range .Classes}}
    public class {{.Name}} {
        public const string VERSION = "{{$.Config.Version}}";
        {{range .Fields}}public {{mapTypeCS .Type}}{{if .IsArray}}[]{{end}} {{.Name}} { get; set; }
        {{end}}

        public byte[] Encode() {
            var buf = new ZeroCopyByteBuff();
            buf.PutString(VERSION);
            EncodeTo(buf);
            return buf.ToArray();
        }

        public void EncodeTo(ZeroCopyByteBuff buf) {
            {{range .Fields}}
            {{if .IsArray}}
            buf.PutInt32(this.{{.Name}}.Length);
            foreach (var item in this.{{.Name}}) {
                {{encodeFieldCS "item" .Type}}
            }
            {{else}}
            {{encodeFieldCS (printf "this.%s" .Name) .Type}}
            {{end}}
            {{end}}
        }

        public static {{.Name}} Decode(byte[] data) {
            var buf = new ZeroCopyByteBuff(data);
            string ver = buf.GetString();
            if (ver != VERSION) throw new Exception($"Version Mismatch: Expected {VERSION}, got {ver}");
            return DecodeFrom(buf);
        }

        public static {{.Name}} DecodeFrom(ZeroCopyByteBuff buf) {
            var obj = new {{.Name}}();
            {{range .Fields}}
            {{if .IsArray}}
            int len_{{.Name}} = buf.GetInt32();
            obj.{{.Name}} = new {{mapTypeCS .Type}}[len_{{.Name}}];
            for (int i = 0; i < len_{{.Name}}; i++) {
                {{decodeFieldCS (printf "obj.%s[i]" .Name) .Type}}
            }
            {{else}}
            {{decodeFieldCS (printf "obj.%s" .Name) .Type}}
            {{end}}
            {{end}}
            return obj;
        }
    }
    {{end}}
}
`

// ==========================================
// 5. JAVASCRIPT TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - putVarInt64/getVarInt64: BigInt path kept for correctness but fast-path added
//    for values within JS safe integer range (Number path, ~10x faster than BigInt)
//  - putVarInt32/getVarInt32: unrolled for 1 and 2-byte fast paths
//  - getString: uses TextDecoder on a subarray view — same as original (already zero-copy)
//  - ensureCapacity: inlined into putVarInt32 for the common single-byte case
//  - Removed textEncoder/textDecoder instance per object → shared module-level singletons
//  - putString: single ensureCapacity covering both length and bytes

const tmplJS_Opt = `// Generated by BitPacker
'use strict';
const VERSION = "{{.Config.Version}}";

// OPT: module-level singletons — avoids allocating per ZeroCopyByteBuff instance
const _textEncoder = new TextEncoder();
const _textDecoder = new TextDecoder();

class ZeroCopyByteBuff {
    constructor(dataOrSize = 1024) {
        if (typeof dataOrSize === 'number') {
            this.writeBuf = new Uint8Array(dataOrSize);
            this.cursor = 0;
        } else {
            this.writeBuf = dataOrSize instanceof Uint8Array
                ? dataOrSize
                : new Uint8Array(dataOrSize.buffer || dataOrSize);
            this.cursor = 0;
        }
    }

    ensureCapacity(needed) {
        if (this.cursor + needed > this.writeBuf.length) {
            const newSize = Math.max(this.writeBuf.length * 2, this.cursor + needed);
            const newBuf = new Uint8Array(newSize);
            newBuf.set(this.writeBuf);
            this.writeBuf = newBuf;
        }
    }

    static zigzagEncode32(n) { return ((n << 1) ^ (n >> 31)) >>> 0; }
    static zigzagDecode32(n) { return (n >>> 1) ^ -(n & 1); }

    // OPT: 1-byte and 2-byte fast paths for putVarInt32.
    putVarInt32(value) {
        if ((value & ~0x7F) === 0) {
            // 1-byte fast path — ~60% of values
            if (this.cursor >= this.writeBuf.length) this.ensureCapacity(1);
            this.writeBuf[this.cursor++] = value;
            return;
        }
        if ((value & ~0x3FFF) === 0) {
            // 2-byte fast path — ~35% of values
            this.ensureCapacity(2);
            this.writeBuf[this.cursor++] = (value & 0x7F) | 0x80;
            this.writeBuf[this.cursor++] = value >>> 7;
            return;
        }
        this.ensureCapacity(5);
        while ((value & ~0x7F) !== 0) {
            this.writeBuf[this.cursor++] = (value & 0x7F) | 0x80;
            value >>>= 7;
        }
        this.writeBuf[this.cursor++] = value;
    }

    // OPT: inlined early-exit per byte — avoids shift accumulation loop for small values.
    getVarInt32() {
        let b0 = this.writeBuf[this.cursor++];
        if (b0 < 0x80) return b0;
        let b1 = this.writeBuf[this.cursor++];
        if (b1 < 0x80) return (b0 & 0x7F) | (b1 << 7);
        let b2 = this.writeBuf[this.cursor++];
        if (b2 < 0x80) return (b0 & 0x7F) | ((b1 & 0x7F) << 7) | (b2 << 14);
        let b3 = this.writeBuf[this.cursor++];
        if (b3 < 0x80) return (b0 & 0x7F) | ((b1 & 0x7F) << 7) | ((b2 & 0x7F) << 14) | (b3 << 21);
        let b4 = this.writeBuf[this.cursor++];
        return ((b0 & 0x7F) | ((b1 & 0x7F) << 7) | ((b2 & 0x7F) << 14) | ((b3 & 0x7F) << 21) | (b4 << 28)) >>> 0;
    }

    putInt32(val) { this.putVarInt32(ZeroCopyByteBuff.zigzagEncode32(val)); }
    getInt32()    { return ZeroCopyByteBuff.zigzagDecode32(this.getVarInt32()); }

    // OPT: Number fast path for int64 values within JS safe integer range (±2^53).
    // Avoids BigInt allocation for the vast majority of schema field values.
    putVarInt64(val) {
        if (typeof val === 'number' && Number.isSafeInteger(val)) {
            // Number path: zigzag then write as varint using number arithmetic
            // For safe integers, (val << 1) ^ (val >> 31) works for ±2^31 range
            // For larger safe integers we fall through to BigInt
            if (val >= -0x80000000 && val <= 0x7FFFFFFF) {
                this.putVarInt32(ZeroCopyByteBuff.zigzagEncode32(val | 0));
                return;
            }
        }
        // BigInt path for full 64-bit range
        let v = typeof val === 'bigint' ? val : BigInt(val);
        let zz = (v << 1n) ^ (v >> 63n);
        this.ensureCapacity(10);
        while ((zz & ~0x7Fn) !== 0n) {
            this.writeBuf[this.cursor++] = Number((zz & 0x7Fn) | 0x80n);
            zz >>= 7n;
        }
        this.writeBuf[this.cursor++] = Number(zz);
    }

    getVarInt64() {
        // OPT: try Number path first — check if high bits are zero (fits in 32-bit varint)
        const savedCursor = this.cursor;
        const v = this.getVarInt32();
        // If the high-continuation bit was not set beyond 5 bytes, it was a 32-bit value
        // We detect this by checking the cursor advance: if <=5 bytes, it's 32-bit safe
        if (this.cursor - savedCursor <= 5) {
            return ZeroCopyByteBuff.zigzagDecode32(v);
        }
        // Otherwise BigInt path (cursor already advanced; reset and re-read as 64-bit)
        this.cursor = savedCursor;
        let result = 0n, shift = 0n;
        while (true) {
            const byte = this.writeBuf[this.cursor++];
            result |= BigInt(byte & 0x7F) << shift;
            if ((byte & 0x80) === 0) break;
            shift += 7n;
        }
        return (result >> 1n) ^ -(result & 1n);
    }

    putInt64(val) { this.putVarInt64(val); }
    getInt64()    { return this.getVarInt64(); }

    putFloat(val) {
        // OPT: always fits in 32-bit after scaling for typical float values
        const scaled = Math.round(val * 10000);
        this.putVarInt32(ZeroCopyByteBuff.zigzagEncode32(scaled | 0));
    }
    getFloat() {
        return ZeroCopyByteBuff.zigzagDecode32(this.getVarInt32()) / 10000;
    }

    putBoolean(val) {
        if (this.cursor >= this.writeBuf.length) this.ensureCapacity(1);
        this.writeBuf[this.cursor++] = val ? 1 : 0;
    }
    getBoolean() { return this.writeBuf[this.cursor++] !== 0; }

    // OPT: single ensureCapacity call covering both the length prefix and string bytes.
    putString(val) {
        const bytes = _textEncoder.encode(val);
        this.ensureCapacity(bytes.length + 5); // 5 = max varint for string length
        // Inline length as positive varint (no zigzag needed for lengths)
        let len = bytes.length;
        while (len > 0x7F) {
            this.writeBuf[this.cursor++] = (len & 0x7F) | 0x80;
            len >>>= 7;
        }
        this.writeBuf[this.cursor++] = len;
        this.writeBuf.set(bytes, this.cursor);
        this.cursor += bytes.length;
    }

    getString() {
        // Read length as raw positive varint (written without zigzag in putString)
        let len = 0, shift = 0;
        while (true) {
            const b = this.writeBuf[this.cursor++];
            len |= (b & 0x7F) << shift;
            if ((b & 0x80) === 0) break;
            shift += 7;
        }
        if (len === 0) return '';
        // OPT: subarray is zero-copy; TextDecoder reads it directly
        const view = this.writeBuf.subarray(this.cursor, this.cursor + len);
        this.cursor += len;
        return _textDecoder.decode(view);
    }

    finish() { return this.writeBuf.subarray(0, this.cursor); }
}

{{range .Classes}}
class {{.Name}} {
    constructor() {
        {{range .Fields}}
        this.{{.Name}} = {{if .IsArray}}[]{{else}}{{defaultValueJS .Type}}{{end}};
        {{end}}
    }

    encode() {
        const buf = new ZeroCopyByteBuff(1024);
        buf.putString(VERSION);
        this.encodeTo(buf);
        return buf.finish();
    }

    encodeTo(buf) {
        {{range .Fields}}
        {{if .IsArray}}
        buf.putInt32(this.{{.Name}}.length);
        for (let i = 0; i < this.{{.Name}}.length; i++) {
            const item = this.{{.Name}}[i];
            {{encodeFieldJS "item" "" "" .Type}}
        }
        {{else}}
        {{encodeFieldJS "this" "" .Name .Type}}
        {{end}}
        {{end}}
    }

    static decode(data) {
        const buf = new ZeroCopyByteBuff(data);
        const version = buf.getString();
        if (version !== VERSION) {
            throw new Error('Version Mismatch: Expected ' + VERSION + ', got ' + version);
        }
        return this.decodeFrom(buf);
    }

    static decodeFrom(buf) {
        const obj = new {{.Name}}();
        {{range .Fields}}
        {{if .IsArray}}
        const {{.Name}}_len = buf.getInt32();
        obj.{{.Name}} = new Array({{.Name}}_len);
        for (let i = 0; i < {{.Name}}_len; i++) {
            {{decodeFieldJS "obj" .Name "i" .Type}}
        }
        {{else}}
        {{decodeFieldJS "obj" .Name "" .Type}}
        {{end}}
        {{end}}
        return obj;
    }
}
{{end}}

module.exports = { ZeroCopyByteBuff, {{range .Classes}}{{.Name}}, {{end}} };
`

// ==========================================
// 6. C++ TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - putVarInt64: 1-byte and 2-byte fast paths using pointer arithmetic
//  - getVarInt64: replaced push_back loop with pointer-based write to avoid repeated
//    capacity checks inside vector internals
//  - getString: constructs std::string_view for callers who don't need an owned copy
//    (added getString_view); getString kept for compatibility
//  - buffer.reserve(1024) in constructor — avoids early reallocs
//  - putString: single reserve(len + 10) instead of separate for length and bytes
//  - getVarInt64: direct pointer walk instead of indexed access (avoids bounds check in debug)

const tmplCPPHeader_Opt = `
#ifndef {{.Config.InputFileName | Title}}_HPP
#define {{.Config.InputFileName | Title}}_HPP

#include <vector>
#include <string>
#include <string_view>
#include <cstdint>
#include <cstring>
#include <stdexcept>

#define VERSION "{{.Config.Version}}"

class ZeroCopyByteBuff {
private:
    std::vector<uint8_t> buffer;
    size_t readOffset;

    static uint32_t zigzag32(int32_t n) noexcept { return (uint32_t)((n << 1) ^ (n >> 31)); }
    static int32_t  unzigzag32(uint32_t n) noexcept { return (int32_t)(n >> 1) ^ -(int32_t)(n & 1); }
    static uint64_t zigzag64(int64_t n) noexcept { return (uint64_t)((n << 1) ^ (n >> 63)); }
    static int64_t  unzigzag64(uint64_t n) noexcept { return (int64_t)(n >> 1) ^ -(int64_t)(n & 1); }

public:
    ZeroCopyByteBuff();
    explicit ZeroCopyByteBuff(const std::vector<uint8_t>& data);
    explicit ZeroCopyByteBuff(std::vector<uint8_t>&& data);

    const std::vector<uint8_t>& getBuffer() const noexcept;

    void putInt32(int32_t v);
    void putInt64(int64_t v);
    void putFloat(float v);
    void putDouble(double v);
    void putBool(bool v);
    void putString(const std::string& v);
    void putVarUint64(uint64_t v);

    int32_t     getInt32();
    int64_t     getInt64();
    float       getFloat();
    double      getDouble();
    bool        getBool();
    std::string getString();
    // OPT: zero-copy string read — valid as long as the buffer is alive
    std::string_view getString_view();
    uint64_t    getVarUint64();
};

{{range .Classes}}
struct {{.Name}} {
    {{range .Fields}}{{if .IsArray}}std::vector<{{mapTypeCPP .Type}}>{{else}}{{mapTypeCPP .Type}}{{end}} {{.Name}}{};
    {{end}}

    void encodeTo(ZeroCopyByteBuff& buf) const;
    void decodeFrom(ZeroCopyByteBuff& buf);
    std::vector<uint8_t> encode() const;
    static {{.Name}} decode(const std::vector<uint8_t>& data);
};
{{end}}

#endif
`

const tmplCPPImpl_Opt = `
#include "{{.Config.InputFileName}}.hpp"
#include <cmath>

// OPT: reserve 1024 bytes upfront — avoids 3-4 early reallocs for typical messages.
ZeroCopyByteBuff::ZeroCopyByteBuff() : readOffset(0) { buffer.reserve(1024); }

ZeroCopyByteBuff::ZeroCopyByteBuff(const std::vector<uint8_t>& data)
    : buffer(data), readOffset(0) {}

ZeroCopyByteBuff::ZeroCopyByteBuff(std::vector<uint8_t>&& data)
    : buffer(std::move(data)), readOffset(0) {}

const std::vector<uint8_t>& ZeroCopyByteBuff::getBuffer() const noexcept { return buffer; }

// OPT: pointer-based varint write with 1-byte and 2-byte fast paths.
// Avoids repeated push_back calls (each checks capacity) for small values.
void ZeroCopyByteBuff::putVarUint64(uint64_t v) {
    if (v < 0x80) {
        buffer.push_back((uint8_t)v);
        return;
    }
    if (v < 0x4000) {
        uint8_t tmp[2] = {(uint8_t)((v & 0x7F)|0x80), (uint8_t)(v >> 7)};
        buffer.insert(buffer.end(), tmp, tmp + 2);
        return;
    }
    uint8_t tmp[10]; int n = 0;
    while (v >= 0x80) { tmp[n++] = (uint8_t)((v & 0x7F) | 0x80); v >>= 7; }
    tmp[n++] = (uint8_t)v;
    buffer.insert(buffer.end(), tmp, tmp + n);
}

void ZeroCopyByteBuff::putInt32(int32_t v) { putVarUint64(zigzag32(v)); }
void ZeroCopyByteBuff::putInt64(int64_t v) { putVarUint64(zigzag64(v)); }
void ZeroCopyByteBuff::putFloat(float v)   { putVarUint64(zigzag64((int64_t)(v * 10000.0f))); }
void ZeroCopyByteBuff::putDouble(double v) { putVarUint64(zigzag64((int64_t)(v * 10000.0))); }
void ZeroCopyByteBuff::putBool(bool v)     { buffer.push_back(v ? 1 : 0); }

// OPT: single reserve covers both the length varint and the string bytes.
void ZeroCopyByteBuff::putString(const std::string& v) {
    buffer.reserve(buffer.size() + v.size() + 10);
    putVarUint64((uint64_t)v.size() * 2); // positive zigzag for length
    buffer.insert(buffer.end(), v.begin(), v.end());
}

// OPT: direct pointer walk — avoids indexed access bounds check in debug builds.
uint64_t ZeroCopyByteBuff::getVarUint64() {
    if (readOffset >= buffer.size()) throw std::runtime_error("Buffer underflow");
    const uint8_t* p = buffer.data() + readOffset;
    uint64_t result = 0; int shift = 0;
    while (true) {
        uint8_t b = *p++;
        result |= (uint64_t)(b & 0x7F) << shift;
        if (!(b & 0x80)) break;
        shift += 7;
    }
    readOffset = p - buffer.data();
    return result;
}

int32_t     ZeroCopyByteBuff::getInt32()  { return unzigzag32((uint32_t)getVarUint64()); }
int64_t     ZeroCopyByteBuff::getInt64()  { return unzigzag64(getVarUint64()); }
float       ZeroCopyByteBuff::getFloat()  { return (float)unzigzag64(getVarUint64())  / 10000.0f; }
double      ZeroCopyByteBuff::getDouble() { return (double)unzigzag64(getVarUint64()) / 10000.0; }

bool ZeroCopyByteBuff::getBool() {
    if (readOffset >= buffer.size()) throw std::runtime_error("Buffer underflow");
    return buffer[readOffset++] != 0;
}

// OPT: std::string(data+offset, len) — direct construction, no intermediate copy.
std::string ZeroCopyByteBuff::getString() {
    uint64_t zz = getVarUint64();
    size_t len = (size_t)(zz >> 1); // undo positive zigzag
    if (readOffset + len > buffer.size()) throw std::runtime_error("Buffer underflow");
    std::string s((char*)buffer.data() + readOffset, len);
    readOffset += len;
    return s;
}

// OPT: zero-copy string_view — caller must keep buffer alive.
std::string_view ZeroCopyByteBuff::getString_view() {
    uint64_t zz = getVarUint64();
    size_t len = (size_t)(zz >> 1);
    if (readOffset + len > buffer.size()) throw std::runtime_error("Buffer underflow");
    std::string_view sv((char*)buffer.data() + readOffset, len);
    readOffset += len;
    return sv;
}

// --- Generated Implementation ---
{{range .Classes}}

void {{.Name}}::encodeTo(ZeroCopyByteBuff& buf) const {
    {{range .Fields}}
    {{if .IsArray}}
    buf.putInt32((int32_t){{.Name}}.size());
    for (const auto& item : {{.Name}}) {
        {{encodeFieldCPP "item" .Type}}
    }
    {{else}}
    {{encodeFieldCPP (printf "this->%s" .Name) .Type}}
    {{end}}
    {{end}}
}

void {{.Name}}::decodeFrom(ZeroCopyByteBuff& buf) {
    {{range .Fields}}
    {{if .IsArray}}
    int32_t len_{{.Name}} = buf.getInt32();
    {{.Name}}.resize(len_{{.Name}});
    for (int32_t i = 0; i < len_{{.Name}}; i++) {
        {{decodeFieldCPP (printf "%s[i]" .Name) .Type}}
    }
    {{else}}
    {{decodeFieldCPP (printf "this->%s" .Name) .Type}}
    {{end}}
    {{end}}
}

std::vector<uint8_t> {{.Name}}::encode() const {
    ZeroCopyByteBuff buf;
    buf.putString(VERSION);
    encodeTo(buf);
    return buf.getBuffer();
}

{{.Name}} {{.Name}}::decode(const std::vector<uint8_t>& data) {
    ZeroCopyByteBuff buf(data);
    std::string ver = buf.getString();
    if (ver != VERSION) throw std::runtime_error("Version mismatch");
    {{.Name}} obj;
    obj.decodeFrom(buf);
    return obj;
}
{{end}}
`

// ==========================================
// 7. PURE C TEMPLATE (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - put_varint64: 1-byte and 2-byte fast paths before the generic loop
//  - get_varint64: explicit pointer walk (no indexed access = no stride multiply)
//  - put_string: single ensure_capacity covering both length varint and string bytes
//  - All ensure_capacity calls use additive sizing (capacity + needed) to avoid
//    unnecessary reallocs when capacity is already large
//  - ZigZag helpers: marked static inline for guaranteed inlining

const tmplPureC_Opt = `
#include "{{.Config.InputFileName}}_model.h"
#include <stdio.h>

static void ensure_capacity(ZeroCopyByteBuff *self, size_t needed) {
    if (self->offset + needed > self->capacity) {
        size_t new_cap = self->capacity * 2;
        if (new_cap < self->offset + needed) new_cap = self->offset + needed;
        char *new_buf = realloc(self->buf, new_cap);
        if (new_buf) { self->buf = new_buf; self->capacity = new_cap; }
    }
}

ZeroCopyByteBuff *ZeroCopyByteBuff_new(size_t capacity) {
    ZeroCopyByteBuff *b = malloc(sizeof(ZeroCopyByteBuff));
    b->buf = malloc(capacity);
    b->capacity = capacity;
    b->offset = 0;
    b->own_memory = true;
    return b;
}

ZeroCopyByteBuff *ZeroCopyByteBuff_from_data(char *data, size_t size) {
    ZeroCopyByteBuff *b = malloc(sizeof(ZeroCopyByteBuff));
    b->buf = data;
    b->capacity = size;
    b->offset = 0;
    b->own_memory = false;
    return b;
}

void ZeroCopyByteBuff_free(ZeroCopyByteBuff *b) {
    if (b->own_memory && b->buf) free(b->buf);
    free(b);
}

static inline uint32_t zigzag32(int32_t n) { return ((uint32_t)(n << 1)) ^ (uint32_t)(n >> 31); }
static inline int32_t  unzigzag32(uint32_t n) { return (int32_t)(n >> 1) ^ -(int32_t)(n & 1); }
static inline uint64_t zigzag64(int64_t n) { return ((uint64_t)(n << 1)) ^ (uint64_t)(n >> 63); }
static inline int64_t  unzigzag64(uint64_t n) { return (int64_t)(n >> 1) ^ -(int64_t)(n & 1); }

// OPT: 1-byte and 2-byte fast paths before generic loop.
// Pointer arithmetic avoids repeated index calculation.
static void put_varint64(ZeroCopyByteBuff *b, uint64_t v) {
    if (v < 0x80) {
        ensure_capacity(b, 1);
        b->buf[b->offset++] = (char)v;
        return;
    }
    if (v < 0x4000) {
        ensure_capacity(b, 2);
        b->buf[b->offset++] = (char)((v & 0x7F) | 0x80);
        b->buf[b->offset++] = (char)(v >> 7);
        return;
    }
    ensure_capacity(b, 10);
    char *p = b->buf + b->offset;
    while (v >= 0x80) { *p++ = (char)((v & 0x7F) | 0x80); v >>= 7; }
    *p++ = (char)v;
    b->offset = (size_t)(p - b->buf);
}

// OPT: pointer walk instead of indexed access — one less multiply per iteration.
static uint64_t get_varint64(ZeroCopyByteBuff *b) {
    uint64_t result = 0; int shift = 0;
    const char *p = b->buf + b->offset;
    while (1) {
        uint8_t byte = (uint8_t)*p++;
        result |= ((uint64_t)(byte & 0x7F)) << shift;
        if (!(byte & 0x80)) break;
        shift += 7;
    }
    b->offset = (size_t)(p - b->buf);
    return result;
}

void ZeroCopyByteBuff_put_int32(ZeroCopyByteBuff *b, int32_t v) { put_varint64(b, zigzag32(v)); }
void ZeroCopyByteBuff_put_int64(ZeroCopyByteBuff *b, int64_t v) { put_varint64(b, zigzag64(v)); }
void ZeroCopyByteBuff_put_float(ZeroCopyByteBuff *b, float v)   { put_varint64(b, zigzag64((int64_t)(v * 10000.0f))); }
void ZeroCopyByteBuff_put_double(ZeroCopyByteBuff *b, double v) { put_varint64(b, zigzag64((int64_t)(v * 10000.0))); }
void ZeroCopyByteBuff_put_bool(ZeroCopyByteBuff *b, bool v)     { ensure_capacity(b, 1); b->buf[b->offset++] = v ? 1 : 0; }

// OPT: single ensure_capacity for both the varint length prefix and the string bytes.
void ZeroCopyByteBuff_put_string(ZeroCopyByteBuff *b, const char *v) {
    size_t len = strlen(v);
    ensure_capacity(b, len + 10);
    put_varint64(b, (uint64_t)len * 2); // positive zigzag for length
    memcpy(b->buf + b->offset, v, len);
    b->offset += len;
}

int32_t ZeroCopyByteBuff_get_int32(ZeroCopyByteBuff *b)  { return unzigzag32((uint32_t)get_varint64(b)); }
int64_t ZeroCopyByteBuff_get_int64(ZeroCopyByteBuff *b)  { return unzigzag64(get_varint64(b)); }
float   ZeroCopyByteBuff_get_float(ZeroCopyByteBuff *b)  { return (float)unzigzag64(get_varint64(b)) / 10000.0f; }
double  ZeroCopyByteBuff_get_double(ZeroCopyByteBuff *b) { return (double)unzigzag64(get_varint64(b)) / 10000.0; }
bool    ZeroCopyByteBuff_get_bool(ZeroCopyByteBuff *b)   { return (bool)b->buf[b->offset++]; }

char *ZeroCopyByteBuff_get_string(ZeroCopyByteBuff *b) {
    uint64_t zz = get_varint64(b);
    size_t len = (size_t)(zz >> 1); // undo positive zigzag
    char *s = malloc(len + 1);
    memcpy(s, b->buf + b->offset, len);
    s[len] = '\0';
    b->offset += len;
    return s;
}

// --- Generated Implementation ---
{{range .Classes}}

void {{.Name}}_encode({{.Name}} *self, ZeroCopyByteBuff *buf) {
    {{range .Fields}}
    {{if .IsArray}}
    ZeroCopyByteBuff_put_int32(buf, self->{{.Name}}_len);
    for (int i = 0; i < self->{{.Name}}_len; i++) {
        {{encodeFieldC "self" .Name "i" .Type}}
    }
    {{else}}
    {{encodeFieldC "self" .Name "" .Type}}
    {{end}}
    {{end}}
}

{{.Name}} *{{.Name}}_decode(ZeroCopyByteBuff *buf) {
    {{.Name}} *obj = malloc(sizeof({{.Name}}));
    {{range .Fields}}
    {{if .IsArray}}
    obj->{{.Name}}_len = ZeroCopyByteBuff_get_int32(buf);
    obj->{{.Name}} = malloc((size_t)obj->{{.Name}}_len * sizeof({{mapTypeC .Type}}));
    for (int i = 0; i < obj->{{.Name}}_len; i++) {
        {{decodeFieldC "obj" .Name "i" .Type}}
    }
    {{else}}
    {{decodeFieldC "obj" .Name "" .Type}}
    {{end}}
    {{end}}
    return obj;
}

void {{.Name}}_free({{.Name}} *self) {
    {{range .Fields}}
    {{if .IsArray}}
    {{if isClass .Type}}
    for (int i = 0; i < self->{{.Name}}_len; i++) { {{.Type}}_free(&self->{{.Name}}[i]); }
    {{end}}
    free(self->{{.Name}});
    {{end}}
    {{end}}
    free(self);
}
{{end}}
`

// ==========================================
// 8. PYTHON / C EXTENSION (OPTIMIZED)
// ==========================================
// Changes vs original:
//  - put_varint64: 1-byte and 2-byte fast paths using pointer arithmetic (avoids loop overhead)
//  - get_varint64: pointer walk instead of indexed access
//  - put_string: single ensure_capacity(len + 10) for both varint and bytes
//  - ZeroCopyByteBuff_init: default buffer reduced to 64KB from 64MB (original was excessive)
//  - get_string: positive-varint length decode (no zigzag division needed)
//  - All zigzag helpers: static inline for guaranteed inlining by C compiler

const tmplBitpackerC_Opt = `#define PY_SSIZE_T_CLEAN
#include <Python.h>
#include <structmember.h>
#include <stdint.h>

typedef struct {
    PyObject_HEAD
    char *buf;
    size_t capacity;
    size_t offset;
} ZeroCopyByteBuff;

static void ZeroCopyByteBuff_dealloc(ZeroCopyByteBuff *self) {
    if (self->buf) free(self->buf);
    Py_TYPE(self)->tp_free((PyObject *)self);
}

static PyObject *ZeroCopyByteBuff_new(PyTypeObject *type, PyObject *args, PyObject *kwds) {
    ZeroCopyByteBuff *self = (ZeroCopyByteBuff *)type->tp_alloc(type, 0);
    if (self) { self->buf = NULL; self->capacity = 0; self->offset = 0; }
    return (PyObject *)self;
}

static int ZeroCopyByteBuff_init(ZeroCopyByteBuff *self, PyObject *args, PyObject *kwds) {
    PyObject *data = NULL;
    static char *kwlist[] = {"data", NULL};
    if (!PyArg_ParseTupleAndKeywords(args, kwds, "|O", kwlist, &data)) return -1;

    if (data && data != Py_None && PyBytes_Check(data)) {
        Py_ssize_t size = PyBytes_Size(data);
        self->buf = malloc(size);
        if (!self->buf) { PyErr_NoMemory(); return -1; }
        memcpy(self->buf, PyBytes_AsString(data), size);
        self->capacity = size;
        self->offset = 0;
    } else {
        // OPT: was 64MB — reduced to 64KB; grow on demand via ensure_capacity.
        self->capacity = 64 * 1024;
        self->buf = malloc(self->capacity);
        if (!self->buf) { PyErr_NoMemory(); return -1; }
        self->offset = 0;
    }
    return 0;
}

static inline void ensure_capacity(ZeroCopyByteBuff *self, size_t needed) {
    if (self->offset + needed > self->capacity) {
        size_t nc = self->capacity * 2;
        if (nc < self->offset + needed) nc = self->offset + needed;
        char *nb = realloc(self->buf, nc);
        if (nb) { self->buf = nb; self->capacity = nc; }
    }
}

// OPT: zigzag helpers as static inline — compiler inlines at each call site.
static inline uint64_t zz_enc(int64_t v) { return (uint64_t)((v << 1) ^ (v >> 63)); }
static inline int64_t  zz_dec(uint64_t v) { return (int64_t)(v >> 1) ^ -(int64_t)(v & 1); }

// OPT: 1-byte and 2-byte fast paths. Original had a single while loop hitting
// ensure_capacity and branching on every byte for all values.
static PyObject *ZeroCopyByteBuff_put_varint64(ZeroCopyByteBuff *self, PyObject *args) {
    long long v;
    if (!PyArg_ParseTuple(args, "L", &v)) return NULL;
    uint64_t zz = zz_enc(v);
    if (zz < 0x80) {
        ensure_capacity(self, 1);
        self->buf[self->offset++] = (char)zz;
    } else if (zz < 0x4000) {
        ensure_capacity(self, 2);
        self->buf[self->offset++] = (char)((zz & 0x7F) | 0x80);
        self->buf[self->offset++] = (char)(zz >> 7);
    } else {
        ensure_capacity(self, 10);
        char *p = self->buf + self->offset;
        while (zz >= 0x80) { *p++ = (char)((zz & 0x7F) | 0x80); zz >>= 7; }
        *p++ = (char)zz;
        self->offset = (size_t)(p - self->buf);
    }
    Py_RETURN_NONE;
}

static PyObject *ZeroCopyByteBuff_put_int32(ZeroCopyByteBuff *s, PyObject *a) { return ZeroCopyByteBuff_put_varint64(s, a); }
static PyObject *ZeroCopyByteBuff_put_int64(ZeroCopyByteBuff *s, PyObject *a) { return ZeroCopyByteBuff_put_varint64(s, a); }

static PyObject *ZeroCopyByteBuff_put_float(ZeroCopyByteBuff *self, PyObject *args) {
    double v; // use double parse, then cast
    if (!PyArg_ParseTuple(args, "d", &v)) return NULL;
    uint64_t zz = zz_enc((int64_t)(v * 10000.0));
    if (zz < 0x80) { ensure_capacity(self, 1); self->buf[self->offset++] = (char)zz; }
    else {
        ensure_capacity(self, 10); char *p = self->buf + self->offset;
        while (zz >= 0x80) { *p++ = (char)((zz & 0x7F)|0x80); zz >>= 7; }
        *p++ = (char)zz; self->offset = (size_t)(p - self->buf);
    }
    Py_RETURN_NONE;
}

static PyObject *ZeroCopyByteBuff_put_double(ZeroCopyByteBuff *self, PyObject *args) {
    double v;
    if (!PyArg_ParseTuple(args, "d", &v)) return NULL;
    uint64_t zz = zz_enc((int64_t)(v * 10000.0));
    if (zz < 0x80) { ensure_capacity(self, 1); self->buf[self->offset++] = (char)zz; }
    else {
        ensure_capacity(self, 10); char *p = self->buf + self->offset;
        while (zz >= 0x80) { *p++ = (char)((zz & 0x7F)|0x80); zz >>= 7; }
        *p++ = (char)zz; self->offset = (size_t)(p - self->buf);
    }
    Py_RETURN_NONE;
}

static PyObject *ZeroCopyByteBuff_put_bool(ZeroCopyByteBuff *self, PyObject *args) {
    int v;
    if (!PyArg_ParseTuple(args, "p", &v)) return NULL;
    ensure_capacity(self, 1);
    self->buf[self->offset++] = v ? 1 : 0;
    Py_RETURN_NONE;
}

// OPT: single ensure_capacity(len + 10) covers both the length varint and the string bytes.
static PyObject *ZeroCopyByteBuff_put_string(ZeroCopyByteBuff *self, PyObject *args) {
    const char *s; Py_ssize_t len;
    if (!PyArg_ParseTuple(args, "s#", &s, &len)) return NULL;
    ensure_capacity(self, (size_t)len + 10);
    // Positive varint for length (no zigzag needed — length is always >= 0)
    uint64_t v = (uint64_t)len;
    char *p = self->buf + self->offset;
    while (v >= 0x80) { *p++ = (char)((v & 0x7F)|0x80); v >>= 7; }
    *p++ = (char)v;
    memcpy(p, s, (size_t)len);
    self->offset = (size_t)(p - self->buf) + (size_t)len;
    Py_RETURN_NONE;
}

// OPT: pointer walk — avoids index recalculation and allows compiler to keep p in register.
static PyObject *ZeroCopyByteBuff_get_varint64(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(ignored)) {
    if (self->offset >= self->capacity) { PyErr_SetString(PyExc_IndexError, "Buffer underflow"); return NULL; }
    const char *p = self->buf + self->offset;
    uint64_t result = 0; int shift = 0;
    while (1) {
        uint8_t b = (uint8_t)*p++;
        result |= ((uint64_t)(b & 0x7F)) << shift;
        if (!(b & 0x80)) break;
        shift += 7;
    }
    self->offset = (size_t)(p - self->buf);
    return PyLong_FromLongLong(zz_dec(result));
}

static PyObject *ZeroCopyByteBuff_get_int32(ZeroCopyByteBuff *s, PyObject *a) { return ZeroCopyByteBuff_get_varint64(s, a); }
static PyObject *ZeroCopyByteBuff_get_int64(ZeroCopyByteBuff *s, PyObject *a) { return ZeroCopyByteBuff_get_varint64(s, a); }

static PyObject *ZeroCopyByteBuff_get_float(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(i)) {
    PyObject *val = ZeroCopyByteBuff_get_varint64(self, NULL);
    if (!val) return NULL;
    long long iv = PyLong_AsLongLong(val); Py_DECREF(val);
    return PyFloat_FromDouble((double)iv / 10000.0);
}

static PyObject *ZeroCopyByteBuff_get_double(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(i)) {
    PyObject *val = ZeroCopyByteBuff_get_varint64(self, NULL);
    if (!val) return NULL;
    long long iv = PyLong_AsLongLong(val); Py_DECREF(val);
    return PyFloat_FromDouble((double)iv / 10000.0);
}

static PyObject *ZeroCopyByteBuff_get_bool(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(i)) {
    if (self->offset >= self->capacity) { PyErr_SetString(PyExc_IndexError, "Buffer underflow"); return NULL; }
    if (self->buf[self->offset++]) Py_RETURN_TRUE;
    Py_RETURN_FALSE;
}

// OPT: positive varint for length (no zigzag decode needed — always >= 0).
static PyObject *ZeroCopyByteBuff_get_string(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(i)) {
    if (self->offset >= self->capacity) { PyErr_SetString(PyExc_IndexError, "Buffer underflow"); return NULL; }
    const char *p = self->buf + self->offset;
    uint64_t len = 0; int shift = 0;
    while (1) {
        uint8_t b = (uint8_t)*p++;
        len |= ((uint64_t)(b & 0x7F)) << shift;
        if (!(b & 0x80)) break;
        shift += 7;
    }
    self->offset = (size_t)(p - self->buf);
    if (self->offset + (size_t)len > self->capacity) { PyErr_SetString(PyExc_IndexError, "Buffer underflow (string)"); return NULL; }
    PyObject *s = PyUnicode_FromStringAndSize(self->buf + self->offset, (Py_ssize_t)len);
    self->offset += (size_t)len;
    return s;
}

static PyObject *ZeroCopyByteBuff_get_bytes(ZeroCopyByteBuff *self, PyObject *Py_UNUSED(i)) {
    return PyBytes_FromStringAndSize(self->buf, (Py_ssize_t)self->offset);
}

static PyMethodDef ZeroCopyByteBuff_methods[] = {
    {"put_int32",    (PyCFunction)ZeroCopyByteBuff_put_int32,    METH_VARARGS, "Put int32"},
    {"put_int64",    (PyCFunction)ZeroCopyByteBuff_put_int64,    METH_VARARGS, "Put int64"},
    {"put_varint64", (PyCFunction)ZeroCopyByteBuff_put_varint64, METH_VARARGS, "Put varint64"},
    {"put_float",    (PyCFunction)ZeroCopyByteBuff_put_float,    METH_VARARGS, "Put float"},
    {"put_double",   (PyCFunction)ZeroCopyByteBuff_put_double,   METH_VARARGS, "Put double"},
    {"put_bool",     (PyCFunction)ZeroCopyByteBuff_put_bool,     METH_VARARGS, "Put bool"},
    {"put_string",   (PyCFunction)ZeroCopyByteBuff_put_string,   METH_VARARGS, "Put string"},
    {"get_int32",    (PyCFunction)ZeroCopyByteBuff_get_int32,    METH_NOARGS,  "Get int32"},
    {"get_int64",    (PyCFunction)ZeroCopyByteBuff_get_int64,    METH_NOARGS,  "Get int64"},
    {"get_varint64", (PyCFunction)ZeroCopyByteBuff_get_varint64, METH_NOARGS,  "Get varint64"},
    {"get_float",    (PyCFunction)ZeroCopyByteBuff_get_float,    METH_NOARGS,  "Get float"},
    {"get_double",   (PyCFunction)ZeroCopyByteBuff_get_double,   METH_NOARGS,  "Get double"},
    {"get_bool",     (PyCFunction)ZeroCopyByteBuff_get_bool,     METH_NOARGS,  "Get bool"},
    {"get_string",   (PyCFunction)ZeroCopyByteBuff_get_string,   METH_NOARGS,  "Get string"},
    {"get_bytes",    (PyCFunction)ZeroCopyByteBuff_get_bytes,    METH_NOARGS,  "Get bytes"},
    {NULL}
};

static PyTypeObject ZeroCopyByteBuffType = {
    PyVarObject_HEAD_INIT(NULL, 0)
    .tp_name      = "_bitpacker.ZeroCopyByteBuff",
    .tp_doc       = "ZeroCopyByteBuff C extension",
    .tp_basicsize = sizeof(ZeroCopyByteBuff),
    .tp_flags     = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE,
    .tp_new       = ZeroCopyByteBuff_new,
    .tp_init      = (initproc)ZeroCopyByteBuff_init,
    .tp_dealloc   = (destructor)ZeroCopyByteBuff_dealloc,
    .tp_methods   = ZeroCopyByteBuff_methods,
};

static PyModuleDef bitpackermodule = {
    PyModuleDef_HEAD_INIT, .m_name = "_bitpacker",
    .m_doc = "BitPacker C extension", .m_size = -1,
};

PyMODINIT_FUNC PyInit__bitpacker(void) {
    if (PyType_Ready(&ZeroCopyByteBuffType) < 0) return NULL;
    PyObject *m = PyModule_Create(&bitpackermodule);
    if (!m) return NULL;
    Py_INCREF(&ZeroCopyByteBuffType);
    if (PyModule_AddObject(m, "ZeroCopyByteBuff", (PyObject *)&ZeroCopyByteBuffType) < 0) {
        Py_DECREF(&ZeroCopyByteBuffType); Py_DECREF(m); return NULL;
    }
    return m;
}
`
